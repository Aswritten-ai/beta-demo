All right, so. I wanted to walk through the way the land, strategy and product on the execution map go to market. This is a check in on the ongoing SIC collective memory project. First, let's do some term disambiguation. It's SIC. That stands for Synthetic Identity Co, which is the legal name. Synthetic Identity Co, which is not user facing anywhere. We have SIC is the company. User facing, collective memory is the RDF narrative source of truth. It's starting to we get. That's not the public facing brand. It's the thing behind the scenes. And that's story collective memory—capital S, capital T, capital O, capital R, capital Y, capital B, capital A, capital S, capital E. No space, collective memory. And the user facing top-level domain is aswritten.ai—A-S-W-R-I-T-T-E-N dot A-I. aswritten.ai. SIC names thus or as written in Latin. i.e. AI that tells you a story, tells you a story as written. So once again, the problem framing is that creating high quality output with AI requires extensive context. Currently models are trying to do that with search and I developed a process for the last couple of years of creating a large, big object at the top of each chat by you know, a very common pattern in prompt engineering is to ask a model to create a plan before executing. Even better is to create multipart detailed specifications, both the plan, but some truth backing it, and as much coverage of the project area as possible. This has been done quite a bit in code repos. You see architecture and agents markdown files and, you know, detailed specs and then multi-agent definitions for how to collaborate across the execution as individual tasks, referencing those specs. So aswritten.ai's mission is to—I'm not the specific language. I'll say it later, but it's to extend software development rigor into strategy, content, and marketing product organizational operations, right? And by doing that, we start with this narrative source of truth, and use it to steer and guide a now much more specific and lightweight prompt execution that fills in this space to light space in the prompt with the organizational, what we call story. It's really the metadata configs or the things inferred from the source of truth. And that allows it to get to use, but would have given us the average of the internet sort of gibberish, you know, if I just asked AI to write a project plan about, you know, my company, I'm going to get a generic thing. If I have memory talking about the company, I'm going to get something more specific. But then that's not controllable. So another framing here is that this is AI memory that is versionable, collaborative, branchable, so Git native, right? And narrative driven. But that also encodes style and conviction and all of these other metrics that we're doing. So that's sort of other products specification stuff, right? So I'm not going to get into all the features and the value propositions right now, but I just want to make sure we have the same wrestling point over what we're offering. So I don't know. As far as where we're at in the product right now, I started I built the initial prototype in n8n that was designed to get running quickly. Some major limitations there that in the code nodes, you can't add you can't install packages. So there's some pretty sketchy code for, you know, things that we would like to do much more robustly like parsing RDF and canonization and, you know, skolemization and that kind of stuff. But also, you know, there is no RDF database in the first place. So, you know, there is no query function as such. It's just returning the RDF snapshot in Turtle and then the LLM is doing whatever it's doing in context. So, I implemented first an n8n agent that has access to the tools directly and the n8n-based tools were aswritten/compile and collective memory/ontology. The ontology is stored in, there's a default ontology stored in the master collective memory repo owned by Synthetic Identity Co GitHub, syntheticidentityco GitHub. And that is optionally overwritten. And a note, we want to do a more complex or extension or inheritance model, perhaps in the future here, by a target repo, right? So the basic structure and there's a large and somewhat bulky and needs to be worked on prompt called storyTWIN, which is the agent prompt that orchestrates the tool use and the idea behind that prompt was you could have a conversation with it and it would first try to enact who it would ask you for a Git repo. So there's a section of the prompt and ask for the repo context as you give it a link. It's parsed and that repo is the repo, but also refs, so you could be on a branch, and then also working directory so based in the repo slash at a particular directory, it'll do that and it'll automatically drop the only directory that you happen period one of those guys a child of the directory we are actually looking for. And, uh. So then that could help context get passed to compile. That's directory and ref aware again and compile takes a sorted list of RDF transactions in the .aswritten directory. And again, there can be multiple .aswritten directories in the hierarchy. And so the, you know, ancestor driven compile, if you start at /advisors, it'll take the transactions in advisors as well as the transactions in its ancestors. Sort all of those, and then replay them to give you a Turtle snapshot. And so that snapshot is considered the source of truth and in the prompt it says use the snapshot as your memory, as your primary memory, secondary being your RAG, and treat that as the steering vector, as the narrative behind us the worldview, behind all of for all output. So the storyTWIN prompt was fundamentally designed to do three things. One is generate backed by a collective memory snapshot. And that's essentially do anything that an LLM normally does, but do it with this source of truth behind it, with its additional structured narrative context. Two is if you provide additional narrative content it will do an extraction. So the idea is that while you're having a conversation, if you provide additional context, while you're working back and forth on areas or questions or whatever else, that context will be extracted using the RDF ontology and then diffed. So those are two separate tools extract and then diff and there's a third tool, TX that translates a diff into a proposed transaction. So those are all aswritten/extract, aswritten/diff, aswritten/tx. And that transaction is presented to the user for confirmation. The idea is that you and this, you know, worked quite well functionally. You iterate with the model in context, to modify that transaction. So, you know, you can say, yes, that looks good, but drop all of the parts about, you know, blank concern. And it does that pretty well. Note, there are no SHACL the validator steps at any part of this. So one missing piece is that behind extraction, we want to define not only validators, but example shapes. And we want to make sure those shapes stay consistent for iteration and that also triples aren't getting hallucinated or changed in that back and forth process. So that's that's one thing. But we'll talk later about how that implementation looks when we talk about the SIC-skills, S-I-C dash S-K-I-L-L-S, MCP server repo. And then the last tool is commit that will take a transaction and commit it to the Git repo and do that on a particular branch as well, right? So the transaction files are always immutable. So we're never overriding a transaction file. This is an append only log. I can add the specifics for the already have structure that describes this transactional system. Currently, the transactions are implemented in SPARQL. We want to move to named graphs that define the add and remove graph for a particular transaction. And as such, also moved to TriG instead of SPARQL. And currently the transaction parsing canonicalization skolemization, normalization is done in some in n8n and we want to move that into Apache Jena Riot, using the CLI, potentially also to Apache Jena or Jena, is it J-E-N-A? Jena that's running inside the Docker Compose. So. So that's the future at it. And so there's one a demo that was just, you know, the n8n agentic interface hosted. Also all the same tools are exposed via an n8n MCP server and that MCP server, I tried with a variety of front ends, including Agent.ai and, you know, stuff like that. I tried it with ChatGPT connector, which works. I did find that the OpenAI tool call seemed to be less effective, but there's a lot more thinking. I find the Anthropic models seem to be much better at doing tools or at least with the prompts that I have set up. But the other big piece is I have deployed aswritten.ai beta app, so let's call this the Open WebUI. I don't know, call it aswritten.ai Open WebUI. And that's now hosted at aswritten.ai. And hosted on Digital Ocean with Docker Compose. And so the next is migrating the hosted n8n into that deployment, adding the SIC-skills MCP server, which is an MCP server designed as a wrapper around bash scripts that's all quad code. So those are gonna end up being much more flexible, self compartmentalizing easily extensible tools for doing RDF operations, extractions, validations, so on and so forth, as well as more flexible Git and file operations, than what we can do in n8n. We're still going to use n8n as the automation and through point to you know sit. Yeah, which we have a variety of things. So the other major feature is stories and the worst, the functionality there is there's a GitHub Action that lives in the target collective memory repo that runs on push to either a transaction file currently .sparql in any folder or a .story file. This is our own file format that takes, defines YAML front matter and AI prompt. And so the idea is that this is exactly the kind of AI prompt you would give to storyTWIN or another AI that is using a compiled collective memory as its source of truth, as its memory. And that it will then write a story from that perspective, right? And so examples of these stories are like a press release using the last transaction. And so one of the front matter is increment true or false. And if you set increment, it will create a new file name for every for the last collective memory transaction. So it allows you to create a new story as state changes. Or you can have one story that is updated as state changes. You can also, the other meaningful front matter, other than ID, title and description and stuff. But the real levers are model and that's an array. And you can define multiple model versions and it will compile that story multiple for those models with the model version appended to the file name. And that gives us the ability to generate stories to write stories triggered and updated by collective memory or changes to the story prompt. And easily introspectable spread of model outputs so we can, you know, see what's working while it's not. The idea is to add an automatic story prompt templator thing. There was some conversation previously that had this been much more complicated, I think. Simple was really good. So like right now, these are just prompts, right. But I'm envisioning as you're having a conversation with storyTWIN or another agentic front end, it will, if you ask it to say, write an article based on the last transaction, it'll first and foremost compile or maybe be included in the results of compile will be the list of stories, story prompts and it'll look at your prompt and see if it's similar to any currently existing stories. And if so, it'll say, there's this currently existing story. Do you want to use that? Or do you want to update that? And you can then, you know, interact with it as such, generate your see, your local version. And after each local generation, I'll say, do you want to update the story with this new prompt? And yeah, and then, you know, we can add and languages try to make that suggestive prompt as good as possible. But I don't think we necessarily need a robot. I'm letting my, I think. It's interesting to consider in the future, how we might be a little pull in our entities or identified subgraphs or, you know, merges and so on and so forth. But I think that's down the road. Right now, open-ended prompt for you to define is great. So that's a big feature. A missing feature is ingestion based on files added to GitHub. So right now, there's two or three file types that make sense to play in GitHub. There's the collective memory/ontology.ttl file, defining a custom ontology. That's optional. This isn't the target repo. There's the list of transactions in .aswritten. And that's again hierarchical across multiple directories. There's a .story file. Oh, another meaningful lever inside the .story file is destination. You can define the output directory, if not, it puts it in a stories directory at that that's the same level of the directory where the story file is found. So the idea is using if you define a story file in a subdirectory, you can have a particular view of the graph and you can have those defined in multiple places. So you get different views of the graph. And you can have all of them compile and write to a single top-level directory, like press or blog or whatever else, right? And but I think the last piece is any additional file that is uploaded that isn't one of these special types should be ingested through the extraction pipeline. Sorry, it's actually called the currently called the individuation pipeline is the combination of extract diff, and then TX and sort of, I'd say that's the result of one result of the individuation pipeline. So. That's the, there's an interactive version, right? That is defined storyTWIN. There's also an n8n workflow version that does all these things sequentially, that does not offer the opportunity to review and edit the specific triples. There is an open question about how we might expose that later, you know, is that just a PR that you can then, you know, comment on, that you can maybe add Claude or Claude on to further manipulate the content of that PR, I think that works for me. So one of the main assumptions, if this isn't obvious, is that this is an organizational content tool designed for programming literate entrepreneurs, designers, developers, consultants, et cetera, right? So, you know, someone that does have and, you know, Git and basic sort of review and potentially AI coding fluency wants to be able to deal with not only content, but the ability to manipulate worldview and see what the results of the change of perspective are. So that ingestion piece is missing. Each of those content items should create transactions, if they are changed, they should create additional transactions that, you know, still have the file name in them. I'm like, have the reference for the previous transaction, so on so forth. So that's one. The next piece is currently the extraction process. The individuation process has this extract and then diff and then TX. And I I'm a little unsure. I think there's a use for that, so I don't want to get rid of that. But there's another version of this that is the so let's let's call it the evolved version. And this is the individuation pipeline evolved variant and instead of so currently extract does not take the snapshot state. It's the input plus the ontology. And then, you know, there's more prompt in this, but basically extract the input and using the provided ontology, like I said, we want to add SHACL validation and examples to make it clear what we're going for. It also includes provenance. Provenance is actually added in the TX step. But evolved version is instead of extract new message onto the ontology and diff two extractions, diff the current state with the new extraction. And there's some language in that diff, that diff is currently handled in the LLM because it has some language about semantic comparison between existing nodes and some, you know, discussion of what an update means, that we want the LLM to be able to infer rather than just doing a set-wise difference between the triples sets. But the evolution version is just gives a single LLM instead of extracting diffs to have one step where you provide the current snapshot, the ontology and the new message and say, produce a transaction that shifts this snapshot as described or as suggested in the message, or, you know, perform an extraction first, and then shifts the snapshot as is suggested by the by this operation, right? So, you know, for example, if I say, actually, I don't want to work with, you know, enterprise companies at all, but we're really working on such and such, right? If we just extract that message, we're going to get, you know, a narrative about now we're with enterprise companies and so on and so forth. And if we do a diff, we're going to add that. And if that diff prompt is rigorous, maybe it's going to pick up some things that need to get removed. But if I treat that prompt as an operation on the snapshot, then we're actually going to get a huge removal and updates to a number of nodes and maybe a few, like one addition. And that's a much more powerful and sort of quickly evolving version. And I think that's actually what we want a lot of the time, if not most of the time. So that's a to do. And then what are the last pieces? Well, I don't know if the last piece, but is uh I don't have a snappy word for this yet, but let's call them repackaged collective memories or a collective memory marketplace. The idea is that, sure, I want to be able to create a custom collective memory. So what I really want to do is so I want to step back for a second into the UX room, right? And I, you know, start talking to this thing, and I say, okay, I want to What we're essentially doing is we're replacing brittle role prompts with this really deep narrative source of truth that can be clearly understood and operated on. So instead of saying, you know, act as if you are a liberal, right? I might say use the NPR collective memory and that will compile a pre-ingested collective memory of, you know, NPR content that represents the narrative source of truth that reflects their perspectives. And then I can now, you know, interact with this LLM as such, right? But that becomes a starting point and I can perform that's now in my local memory and I can perform operations on it, right? So just like we were saying, you can evolve a collective memory, I can say, all right, now, you know, remove references to blank or, you know, change, you know, mentions around said policy to, you know, such and so, right? Or I can add individual pieces of content that would be right. So this is the difference, right? Content is extracted and then diffed. But a prompt in the collective memory chat is going to be treated as an evolution if it describes an operation. And that's a little bit of a trippy part. We're going to need to have a classifier that is going to separate these things out. Or there need to be clearly disambiguated agents that are intended to do different things, right? And so an obvious one is a sort of read only mode, right, that reads from collective memories and writes content. Another is one that does evolution that's designed to evolve a collective memory. If we remove content ingestion from the agent and have that solely as upload so you can upload it if you add a file to the agent, it'll add it to the Git repo and then kick off the ingestion pipeline. But if you are talking to it, it's treated like evolution, either evolution or generation. So, and that would be a read-write mode, which is a more admin mode, right? Well, actually I'm not sure if it's admin. I think there's an open question too, of what operations are admin only. I definitely want to be able to evolve a local collective memory. What I don't want someone who doesn't have write access to the Git repo is write to it. And so actually collective memory and we missed this in the beginning, defaults all access permission to the GitHub role. And so there was a missing piece doing user OAuth for GitHub. Right now, the hack is that it's using a personal access token set in n8n. And that's a personal access token with open access to all repos connected to the collective memory user, the aswritten.ai GitHub user, and so if you want to be able to use your repo, you just need to invite that user that needs to get shifted to GitHub apps with appropriate OAuth and you know, password credentials and so on and so forth. So the idea is, as I'm talking to this thing, I say, okay, start with this perspective, this collective memory. Now I do my operations on it. And I could say, you know, one of the operations I could do is I could say merge with another collective memory or evolve with another collective memory so I could start with NPR and then add, I don't know, the FDA, right? which is sort of a strange combination, but you'd get this sort of middle ground, right? But maybe sort of more, obviously, like I could. So maybe start with my company collective memory where I've been developing pitches and I could evolve with the collective memory for, say, OpenAI, right, which would give me shift in the direction of, you know, frontier model and areas around AI saving the world. And, you know, I can ask sort of whatever, like I can use those as manipulable entities and, you know, say what would use these personas against each other. So the collective memory snapshot becomes a rigorous data intensive, large description of a persona that is operable by LLMs, that they understand intuitively. We don't have to build anything else to give them the ability to do this kind of, you know, persona or perspectival mash with, you know, collective memory and RDF and the provided ontology does a lot to make that something that can be multiplicative in other, you know, manipulatable forms. But the idea is that you could that we want to create a marketplace of or a repository that is very released of these a repository of these perspectives and So I think that will be in the master collective memory repo and there'll be a number of directories, and they won't have where the file stories and they won't have uploaded sources of content that then ingested that upload through the GitHub Action and and then we can also create additional n8n automations that ing that add content that is published to the repo. So, you know, for example, if someone if we want to create a profile of the news outlet, when they create a new article, then we can ingest it. There's some data constraints, for sure, that we probably don't want to ingest everything like that. You know, great example is I want to do a demo for the service company, Crooked Media. They do their podcasts Pod Save America and I want to set up, you know, an example where the transcript slash show notes are automatically added to the repo and ingested. And so you'll see, you know, stories that are automatically updating on change and you'll be able to talk to an agent in the aswritten.ai Open WebUI beta, or elsewhere, and start from this perspective and then, you know, perform your perspectival operations on it, right? Say, you know, of all the misdirection or, you know, so on influence. And those can be, you know, quite long, right? The best way to do this, to do a perspectival operation is to provide a lot of input, right? So this transcript discussion I'm having right now is the perfect way to evolve something. And so this is probably a good example. This transcript is a good example of something we both want to extract from, and then also use as an evolution. So I think the evolution tool wants to first extract and then additionally take the message as a direct manipulation and then perform the sort of, I don't know, diff between the two or merge the resulting two. So that needs to get worked out. But so the questions now are, you know, what needs to get done and in what order. I think I've laid out all of the big pieces into to-dos. If there's something in memory that I haven't mentioned, we use note that, and include a section on what's missing. Please note the edges and things that are, you know, undefined. Please provide. Yeah. You to provide a set of prompts for the development and we're left. I know what I forgot. So currently Open WebUI, the aswritten.ai Open WebUI is set up to auth with Outseta, O-U-T-S-E-T-A. Outseta is the OIDC provider and that is working. And I have plans set up in Outseta. Pricing is not locked down. And currently I have the Open WebUI workspace models set up as the storyTWIN prompt with access to the n8n collective memory MCP tools, and there's an issue where they're calling inefficiently quite a bit. So that's something n8n to investigate. But currently, there is a function that adds the Outseta auth session information—this is the account and plan, et cetera—into the headers of every request and we're then proxying the Open Router API through Helicone, H-E-L-I-C-O-N-E, for metrics monitoring and also usage limiting. And so there's basic usage limiting in right now. But what I'd like to do is set up cost pass through for API spend. And there's some open question about, you know, where all of the different spend is coming from, but it's both the spend from Open WebUI, the model itself, there's then spend from the base model in n8n. Actually, I guess that's, so it's either the agent model in the raw n8n flow or the model defined in the workspace model in Open WebUI. But then there's the model calls in the tools themselves in n8n and that's potentially substantial spend, and there's the tools both behind. And the tools that are accessed by GitHub Actions. And so currently, we could have Actions tools, yeah, ultimately the MCP tools need to receive the Outseta information and there's an issue with the n8n MCP server trigger node that it cannot be set up to accept auth headers. So we're going to need to wrap a new MCP server that calls the n8n tools directly via HTTP and adapt those nodes or those workflows to take webhook post. And. And so that will allow us to track billing and include features and the plans and Helicone. But then the separate pathway to understand billing for custom collective memory repositories. And so those need an API key configured in their environment that then needs to get sent to GitHub webhook that allows it to include the appropriate user headers when calling Open Router proxied through Helicone so we can include that usage in the user's spend. And then the idea is in Outseta to charge somewhat lower base rate, and then bill per usage times percentage multiplier. I think is going to be a better and more approachable usage model, especially for individuals. So that'll let us get people on a platform more quickly. It'll also let me use the Open WebUI as a generic interface and also let people access any of the available Open Router models if they want to just pay the increased per token thing. So I should be able to make some a margin on general API use, as well as the collective memory use and manipulation itself.